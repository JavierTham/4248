{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/miniforge3/envs/cs4248/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Explanation_1</th>\n",
       "      <th>WorkerId</th>\n",
       "      <th>Sentence1_marked_1</th>\n",
       "      <th>Sentence2_marked_1</th>\n",
       "      <th>Sentence1_Highlighted_1</th>\n",
       "      <th>Sentence2_Highlighted_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3416050480.jpg#4r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>the person is not necessarily training his horse</td>\n",
       "      <td>AF0PI3RISB5Q7</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is *training* *his* *horse* for a co...</td>\n",
       "      <td>{}</td>\n",
       "      <td>3,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3416050480.jpg#4r1c</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>One cannot be on a jumping horse cannot be a d...</td>\n",
       "      <td>A36ZT2WFIA2HMF</td>\n",
       "      <td>A person *on* *a* *horse* *jumps* over a brok...</td>\n",
       "      <td>A person *is* *at* *a* *diner,* *ordering* an...</td>\n",
       "      <td>4,2,3,5</td>\n",
       "      <td>2,5,4,3,6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3416050480.jpg#4r1e</td>\n",
       "      <td>entailment</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>a broken down airplane is outdoors</td>\n",
       "      <td>A2GK75ZQTX2RDZ</td>\n",
       "      <td>A person on a horse jumps over *a* *broken* *...</td>\n",
       "      <td>A person is *outdoors,* on a horse.</td>\n",
       "      <td>8,9,10,7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2267923837.jpg#2r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>Just because they are smiling and waving at a ...</td>\n",
       "      <td>A18TOIDG32QICP</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling *at* *their* *parents*</td>\n",
       "      <td>{}</td>\n",
       "      <td>5,3,4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2267923837.jpg#2r1e</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>The children must be present to see them smili...</td>\n",
       "      <td>AEX0YE6TUZRHT</td>\n",
       "      <td>*Children* *smiling* *and* *waving* at camera</td>\n",
       "      <td>There are children *present*</td>\n",
       "      <td>0,1,3,2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pairID     gold_label  \\\n",
       "0  3416050480.jpg#4r1n        neutral   \n",
       "1  3416050480.jpg#4r1c  contradiction   \n",
       "2  3416050480.jpg#4r1e     entailment   \n",
       "3  2267923837.jpg#2r1n        neutral   \n",
       "4  2267923837.jpg#2r1e     entailment   \n",
       "\n",
       "                                           Sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                           Sentence2  \\\n",
       "0  A person is training his horse for a competition.   \n",
       "1      A person is at a diner, ordering an omelette.   \n",
       "2                  A person is outdoors, on a horse.   \n",
       "3                  They are smiling at their parents   \n",
       "4                         There are children present   \n",
       "\n",
       "                                       Explanation_1        WorkerId  \\\n",
       "0   the person is not necessarily training his horse   AF0PI3RISB5Q7   \n",
       "1  One cannot be on a jumping horse cannot be a d...  A36ZT2WFIA2HMF   \n",
       "2                 a broken down airplane is outdoors  A2GK75ZQTX2RDZ   \n",
       "3  Just because they are smiling and waving at a ...  A18TOIDG32QICP   \n",
       "4  The children must be present to see them smili...   AEX0YE6TUZRHT   \n",
       "\n",
       "                                  Sentence1_marked_1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1   A person *on* *a* *horse* *jumps* over a brok...   \n",
       "2   A person on a horse jumps over *a* *broken* *...   \n",
       "3              Children smiling and waving at camera   \n",
       "4      *Children* *smiling* *and* *waving* at camera   \n",
       "\n",
       "                                  Sentence2_marked_1 Sentence1_Highlighted_1  \\\n",
       "0   A person is *training* *his* *horse* for a co...                      {}   \n",
       "1   A person *is* *at* *a* *diner,* *ordering* an...                 4,2,3,5   \n",
       "2                A person is *outdoors,* on a horse.                8,9,10,7   \n",
       "3            They are smiling *at* *their* *parents*                      {}   \n",
       "4                       There are children *present*                 0,1,3,2   \n",
       "\n",
       "  Sentence2_Highlighted_1  \n",
       "0                   3,4,5  \n",
       "1               2,5,4,3,6  \n",
       "2                       3  \n",
       "3                   5,3,4  \n",
       "4                       3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/esnli_train_1.csv') #rmb to train on whole dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameColumnsTrain(df):\n",
    "    return df.rename(columns={'Sentence1': 'premise', 'Sentence2': 'hypothesis', 'Explanation_1': 'explanation'}).drop([\"WorkerId\", \"Sentence1_Highlighted_1\", \"Sentence2_Highlighted_1\"], axis=1)\n",
    "\n",
    "df_cleaned = renameColumnsTrain(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12a79ea5c0d46de94611c6398101c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   250,   621,    15,    10,  5253, 13855,    81,    10,  3187,\n",
      "           159, 16847,     4,     2,     2,   250,   621,    16,  1058,    39,\n",
      "          5253,    13,    10,  1465,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n",
      "Premise: A person on a horse jumps over a broken down airplane.\n",
      "Hypothesis: A person is training his horse for a competition.\n",
      "Explanation: the person is not necessarily training his horse\n",
      "\n",
      "True class: neutral\n",
      "Predicted class: entailment\n"
     ]
    }
   ],
   "source": [
    "# cell for testing the model's output for a single example\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize input\n",
    "premise = df_cleaned['premise'][0]\n",
    "hypothesis = df_cleaned['hypothesis'][0]\n",
    "explanation = df_cleaned['explanation'][0]\n",
    "actual_label = df_cleaned['gold_label'][0]\n",
    "encoded_input = tokenizer.encode_plus(premise, hypothesis, explanation, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = torch.tensor(df_cleaned['gold_label'].replace(label_to_id).tolist())[0]\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n",
    "\n",
    "predicted_class = torch.argmax(output.logits, dim=1)\n",
    "\n",
    "print(f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nExplanation: {explanation}\\n\")\n",
    "print(f\"True class: {actual_label}\")\n",
    "print(f\"Predicted class: {id_to_label[predicted_class.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterNan(df):\n",
    "    return df.dropna()\n",
    "\n",
    "# def tokenize(df):\n",
    "#     return df.apply(lambda x: tokenizer.encode_plus(x['premise'], x['hypothesis'], x['explanation'], padding='max_length', return_tensors='pt'), axis=1)\n",
    "\n",
    "def convert_to_tensors(df):\n",
    "    return torch.tensor(df.values)\n",
    "\n",
    "def encode_labels(df):\n",
    "    return df.apply(lambda x: label_to_id[x])\n",
    "\n",
    "template = \"\"\"\n",
    "Premise: {}\n",
    "Hypothesis: {}\n",
    "Explanation: {}\n",
    "\"\"\"\n",
    "\n",
    "def tokenize(df):\n",
    "    tokenized_batch = []\n",
    "    for _, row in df.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text = template.format(row['premise'], row['hypothesis'], row['explanation']),\n",
    "            # row['premise'], # two ways to encode\n",
    "            # row['hypothesis'], \n",
    "            # row['explanation'],\n",
    "            padding=True,\n",
    "            return_tensors='pt',\n",
    "            # truncation=True\n",
    "        )\n",
    "        tokenized_batch.append(encoded_dict)\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RoBERTA huggingface](https://huggingface.co/FacebookAI/roberta-base#:~:text=RoBERTa%20is%20a%20transformers%20model%20pretrained%20on%20a,to%20generate%20inputs%20and%20labels%20from%20those%20texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3) # <- removed problem_type = multi_label_classification because it is multi-class, not multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# for nvidia GPUs\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = renameColumnsTrain(df)\n",
    "df_cleaned = df_cleaned[:1000]\n",
    "df_cleaned = filterNan(df_cleaned)\n",
    "df_cleaned['gold_label'] = encode_labels(df_cleaned['gold_label'])\n",
    "tokenized_input = tokenize(df_cleaned) # why tokenize the entire df? there is data leakage here\n",
    "actual_labels = convert_to_tensors(df_cleaned['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   250,   621,    15,    10,  5253, 13855,    81,    10,  3187,\n",
       "           159, 16847,     4,    22,   250,   621,    16,  1058,    39,  5253,\n",
       "            13,    10,  1465,    72,   142,     5,   621,    16,    45,  4784,\n",
       "          1058,    39,  5253,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template goes like this:\n",
    "\n",
    "\"A person on a horse jumps over a broken down airplane.\" <mask> \"A person is training his horse for a competition.\" because the person is not necessarily training his horse\n",
    "\n",
    "if model predicts contradict then,\n",
    "\n",
    "\"A person on a horse jumps over a broken down airplane.\" <span style=\"color:red\">contradict</span> \"A person is training his horse for a competition.\" because the person is not necessarily training his horse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = tokenizer.mask_token\n",
    "template = \"\"\"{} \"{}\" because {}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_premise = df_cleaned['premise'][0]\n",
    "sample_hypothesis = df_cleaned['hypothesis'][0]\n",
    "sample_explanation = df_cleaned['explanation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(f'\"{sample_premise}\"',\n",
    "                          template.format(mask_token, sample_hypothesis, sample_explanation),\n",
    "                          padding=\"max_length\",\n",
    "                          truncation=True,\n",
    "                          return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>\"A person on a horse jumps over a broken down airplane.\"</s></s><mask> \"A person is training his horse for a competition.\" because the person is not necessarily training his horse</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.batch_decode(encoded_input.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_input.attention_mask # 1's are for the actual input tokens, 0's are for the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa doesn't have token_type_ids, separate with </s> or tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = encoded_input.values()\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = output.logits.argmax()\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict without training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eSNLIDataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "\n",
    "        self.premise_template = '\"{}\"'\n",
    "        self.hypothesis_explanation_template = '\"{}\" because {}'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.df.iloc[idx,:]\n",
    "        premise = example[\"premise\"]\n",
    "        hypothesis = example[\"hypothesis\"]\n",
    "        explanation = example[\"explanation\"]\n",
    "\n",
    "        premise = self.premise_template.format(premise)\n",
    "        hypothesis = self.hypothesis_explanation_template.format(hypothesis, explanation)\n",
    "\n",
    "        if self.train:\n",
    "            label = example[\"gold_label\"]\n",
    "            return premise, hypothesis, label\n",
    "        \n",
    "        return premise, hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = actual_labels\n",
    "\n",
    "dataset = eSNLIDataset(df_cleaned, train=False) # we're just gonna predict the labels\n",
    "dataloader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        premise, hypothesis = batch\n",
    "        encoded_input = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model(**encoded_input)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions.extend(logits.argmax(dim=-1).cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.49962490622655664, 0.333, 0.16654163540885222)\n"
     ]
    }
   ],
   "source": [
    "print(calc_f1_score(predictions, actual_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# print(tokenized_input)\n",
    "input_ids = [x['input_ids'].squeeze(0) for x in tokenized_input]\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "attention_masks = [x['attention_mask'].squeeze(0) for x in tokenized_input]\n",
    "attention_masks = pad_sequence(attention_masks, batch_first=True)\n",
    "\n",
    "labels = actual_labels\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "loader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_attention_mask = batch_attention_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        predicted_classes = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(predicted_classes)\n",
    "\n",
    "predictions = torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1_score(predicted_classes, actual_labels):\n",
    "    return f1_score(predicted_classes, actual_labels, average='weighted'), f1_score(predicted_classes, actual_labels, average='micro'), f1_score(predicted_classes, actual_labels, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(calc_f1_score(\u001b[43mpredictions\u001b[49m, actual_labels))\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(calc_f1_score(predictions, actual_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Trainer with the wrapped dataset\n",
    "trainer = Trainer(\n",
    "    model=model,                   \n",
    "    args=training_args,            \n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/zp49v82x2694tmc1tz0w2sn00000gn/T/ipykernel_89320/2268945433.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(actual_labels)\n",
      "/Users/dombrr/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80d60fe522a47b49727afe77fc53636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# need to wrap in a dictionary to use the Trainer class\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, tensor_dataset):\n",
    "        self.tensor_dataset = tensor_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, attention_mask, labels = self.tensor_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "input_ids = [x['input_ids'].squeeze(0) for x in tokenized_input]\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "attention_masks = [x['attention_mask'].squeeze(0) for x in tokenized_input]\n",
    "attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n",
    "labels = torch.tensor(actual_labels)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "num_classes = 3\n",
    "labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "#split the dataset into training and validation\n",
    "split = int(len(input_ids) * 0.8)\n",
    "input_ids_train = input_ids[:split]\n",
    "attention_masks_train = attention_masks[:split]\n",
    "labels_one_hot_train = labels_one_hot[:split]\n",
    "\n",
    "input_ids_val = input_ids[split:]\n",
    "attention_masks_val = attention_masks[split:]\n",
    "labels_one_hot_val = labels_one_hot[split:]\n",
    "\n",
    "tensor_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_one_hot_train)\n",
    "\n",
    "dataset = DictDataset(tensor_dataset)\n",
    "\n",
    "validation_tensor_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_one_hot_val)\n",
    "validation_dataset = DictDataset(validation_tensor_dataset)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=10,             \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,                \n",
    "    evaluation_strategy='steps',     \n",
    "    eval_steps=50,                   \n",
    "    save_strategy='epoch',           \n",
    "    save_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the wrapped dataset\n",
    "trainer = Trainer(\n",
    "    model=model,                   \n",
    "    args=training_args,            \n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4615384615384615, 0.3, 0.15384615384615383)\n"
     ]
    }
   ],
   "source": [
    "# predict after training\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataset:\n",
    "        batch_input_ids = batch['input_ids'].unsqueeze(0).to(device)\n",
    "        batch_attention_mask = batch['attention_mask'].unsqueeze(0).to(device)\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits.cpu()\n",
    "\n",
    "        predicted_classes = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(predicted_classes)\n",
    "\n",
    "predictions = torch.stack(predictions)\n",
    "\n",
    "\n",
    "print(calc_f1_score(predictions, actual_labels[split:]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
